# HEAL-Summ-lite: Lightweight Health Summarization Pipeline

This repository contains a Jupyter notebook implementation of a lightweight health text summarization pipeline, inspired by the HEAL-Summ methodology proposed by Dr. Fisher and colleagues ([2025](https://doi.org/10.3389/fpubh.2025.1619274)). The notebook includes code, visualizations, and detailed explanations of the approach, assumptions, results, and limitations.

## Key Features
- Summaries (120-180 words) generated by three local LLMs:
  - Phi‑3.5 Mini (GGUF)
  - Apple OpenELM‑3B (GGUF)
  - FLAN‑T5‑Large (Transformers)
- Readability scores (Flesch Reading Ease) via `textstat`.
- Risk heuristics: missing numbers, missing named entities, overconfident claims.
- Human review rule triggered by readability <50, any risk flag, or length violation.
- Lexical diversity metrics (Distinct‑1/2) via `nltk`.
- Visualizations: bar charts, heatmaps, boxplots.

## Approach & Reasoning
- **Why these models?**  
  - Phi‑3.5 Mini: baseline from HEAL-Summ, accessible summaries.  
  - Apple OpenELM‑3B: privacy-focused, efficient GGUF format.  
  - FLAN‑T5‑Large: instruction-tuned, good for summarization tasks.  
- **Assumptions:**  
  - Summaries should retain key numbers and named entities.  
  - Lightweight heuristics suffice for initial safety screening.  
  - A small dataset (5 texts) demonstrates functionality.

## What Worked / What Didn’t
- **Worked:**  
  - Apple OpenELM‑3B consistently met length targets (120-180 words) and balanced readability with completeness.  
  - Risk heuristics flagged missing numbers (7/15 summaries) and other issues.  
  - Visualizations clearly showed model trade‑offs.  
- **Failure case:**  
  - FLAN‑T5‑Large produced summaries averaging 60-80 words, failing length requirement.  
  - Simple heuristics missed some omitted entities.

## Evaluation / Check
- **Flesch Reading Ease** computed for every summary.  
- **Human review rule:** Escalate if FRE < 50, any risk flag, or length outside 120-180 words.

## Improvements with More Time
- Implement proper named entity recognition (NER) for better accuracy.  
- Verify factual claims against knowledge bases.  
- Test on larger datasets with clinical expert validation.  
- Tune heuristic thresholds to reduce false positives.

## Tool Use Disclosure
- **LLM loading:**  
  - `llama-cpp-python` (Phi‑3.5 Mini, Apple OpenELM‑3B GGUF)  
  - Hugging Face `transformers` + `torch` (FLAN‑T5‑Large)  
- **Evaluation:**  
  - `textstat` (readability)  
  - `nltk` (Distinct‑1/2 lexical diversity)  
- **Data & viz:**  
  - `pandas`, `matplotlib`, `seaborn`, `tqdm`, `IPython.display`  

For a complete walkthrough, including code, results, and detailed discussion, please see the Jupyter notebook (`heal_summ.ipynb`).

Full results available in [`heal_summ_all_models.csv`](Summaries Generated Per Model/heal_summ_all_models.csv)
